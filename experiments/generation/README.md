# Graph Generation Model Evaluation

This directory contains experiments evaluating novelty scores on graphs generated by state-of-the-art generation models.

## Generation Models to Evaluate

### 1. GraphRNN (2018)
- **Paper**: "GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models"
- **Domain**: Molecular graphs (QM9)
- **Approach**: Sequential node/edge generation
- **Source**: Use pre-trained checkpoint from original repository

### 2. GraphVAE (2018)
- **Paper**: "GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders"
- **Domain**: Molecular graphs (QM9)
- **Approach**: Variational autoencoder
- **Source**: Use pre-trained checkpoint

### 3. GraphAF (2020)
- **Paper**: "GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation"
- **Domain**: Molecular graphs (ZINC)
- **Approach**: Normalizing flows
- **Source**: Use pre-trained checkpoint

### 4. DiGress (2022)
- **Paper**: "DiGress: Discrete Denoising Diffusion for Graph Generation"
- **Domain**: Molecular graphs + General graphs
- **Approach**: Diffusion models
- **Source**: Latest state-of-the-art diffusion-based generator

### 5. GDSS (2023)
- **Paper**: "GDSS: Graph Denoising Diffusion for Stochastic Structures"
- **Domain**: Multiple domains
- **Approach**: Score-based diffusion
- **Source**: Most recent approach

## Experimental Protocol

### Phase 1: Generate Graphs
For each model and dataset:
1. Load pre-trained checkpoint
2. Generate 1000 molecular graphs
3. Save generated graphs in NetworkX format

### Phase 2: Evaluate Novelty
For each generated set:
1. Load training corpus (90% of dataset)
2. Initialize GCN with corpus
3. Compute novelty scores for all generated graphs
4. Report statistics (mean, std, distribution)

### Phase 3: Comparative Analysis
Compare generation models:
1. Novelty score distributions
2. Correlation with validity
3. Correlation with uniqueness
4. Component-level breakdown (structural, edge-type, bridging)

## Expected Results

### Hypothesis 1: Validity-Novelty Trade-off
Models optimized for high validity (e.g., constrained generation) will show lower novelty scores.

### Hypothesis 2: Diffusion Models Show Higher Novelty
Recent diffusion-based models (DiGress, GDSS) will exhibit higher novelty due to more exploratory sampling.

### Hypothesis 3: Component Patterns
- **GraphRNN**: High structural novelty (sequential errors accumulate)
- **GraphVAE**: Moderate novelty (interpolation in latent space)
- **GraphAF**: Balanced novelty (flow-based invertibility)
- **DiGress/GDSS**: Highest novelty (stochastic diffusion)

## Files

- `generate_graphrnn.py`: Generate graphs using GraphRNN
- `generate_graphvae.py`: Generate graphs using GraphVAE
- `generate_graphaf.py`: Generate graphs using GraphAF
- `generate_digress.py`: Generate graphs using DiGress
- `generate_gdss.py`: Generate graphs using GDSS
- `evaluate_generated.py`: Evaluate novelty on generated graphs
- `compare_models.py`: Compare novelty across models
- `plot_comparison.py`: Generate comparison figures

## Runtime Estimates

- **Generation**: ~2-4 hours per model (1000 graphs)
- **Evaluation**: ~30 minutes per set
- **Total**: ~10-12 hours for all models

## Output

Results will be saved to:
- `results/generation/graphrnn_qm9.json`
- `results/generation/graphvae_qm9.json`
- `results/generation/graphaf_zinc.json`
- `results/generation/digress_qm9.json`
- `results/generation/gdss_qm9.json`
- `results/generation/comparison.json`

Figures will be generated:
- `paper/figures/figure_generation_comparison.pdf`
- `paper/figures/figure_generation_components.pdf`
