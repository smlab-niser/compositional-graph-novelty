"""
Evaluate novelty scores on graphs generated by various generation models
"""

import argparse
import json
import numpy as np
import pickle
from pathlib import Path
import sys
import time

sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'src'))

from utils.data_loader import load_dataset
from gcn.core import GraphCompositionalNovelty


def load_generated_graphs(filepath):
    """Load generated graphs from file"""
    with open(filepath, 'rb') as f:
        return pickle.load(f)


def evaluate_generation_model(model_name, dataset_name, generated_graphs_path,
                              corpus_size=900, k=3):
    """
    Evaluate novelty of graphs from a generation model

    Args:
        model_name: Name of generation model (e.g., 'GraphRNN', 'GraphVAE')
        dataset_name: Dataset name (e.g., 'qm9', 'zinc')
        generated_graphs_path: Path to pickle file with generated graphs
        corpus_size: Size of training corpus
        k: Motif size
    """

    print(f"{'='*80}")
    print(f"Evaluating {model_name} on {dataset_name}")
    print(f"{'='*80}")

    # Load training corpus
    print(f"\nLoading corpus from {dataset_name}...")
    corpus_graphs, _ = load_dataset(dataset_name, corpus_size=corpus_size, eval_size=0)
    print(f"  Corpus: {len(corpus_graphs)} graphs")

    # Load generated graphs
    print(f"\nLoading generated graphs from {generated_graphs_path}...")
    generated_graphs = load_generated_graphs(generated_graphs_path)
    print(f"  Generated: {len(generated_graphs)} graphs")

    # Initialize GCN
    print(f"\nInitializing GCN (k={k})...")
    start = time.time()
    gcn = GraphCompositionalNovelty(corpus_graphs, k=k)
    init_time = time.time() - start
    print(f"  Initialization time: {init_time:.2f}s")
    print(f"  Unique corpus motifs: {len(gcn.corpus_motifs)}")

    # Evaluate novelty on generated graphs
    print(f"\nEvaluating novelty on {len(generated_graphs)} generated graphs...")
    start = time.time()

    results = {
        'overall': [],
        'structural': [],
        'edge_type': [],
        'bridging': [],
        'details': []
    }

    for i, G in enumerate(generated_graphs):
        if (i + 1) % 100 == 0:
            print(f"  Progress: {i+1}/{len(generated_graphs)}")

        try:
            novelty = gcn.compute_novelty(G)

            results['overall'].append(novelty['overall_novelty'])
            results['structural'].append(novelty['structural_novelty'])
            results['edge_type'].append(novelty['edge_novelty'])  # Fixed key name
            results['bridging'].append(novelty['bridging_novelty'])
            results['details'].append({
                'graph_id': i,
                'n_nodes': G.number_of_nodes(),
                'n_edges': G.number_of_edges(),
                'novelty': novelty
            })
        except Exception as e:
            print(f"    Warning: Failed to evaluate graph {i}: {e}")
            continue

    eval_time = time.time() - start
    print(f"  Evaluation time: {eval_time:.2f}s")

    # Check if we have any results
    if len(results['overall']) == 0:
        print(f"\n{'='*80}")
        print(f"ERROR: No graphs were successfully evaluated!")
        print(f"{'='*80}")
        print("\nPossible issues:")
        print("1. All graphs failed evaluation (check graph format)")
        print("2. Graphs are incompatible with the dataset")
        print("3. Graphs are empty or malformed")
        print("\nPlease check:")
        print("- Graph nodes have required attributes (atom_type for molecules)")
        print("- Graph edges have required attributes (bond_type for molecules)")
        print("- Graphs are non-empty and connected")
        sys.exit(1)

    # Compute statistics
    print(f"\n{'='*80}")
    print(f"Results for {model_name}")
    print(f"{'='*80}")

    stats = {
        'model': model_name,
        'dataset': dataset_name,
        'n_generated': len(generated_graphs),
        'n_evaluated': len(results['overall']),
        'corpus_size': corpus_size,
        'k': k,
        'init_time': init_time,
        'eval_time': eval_time,
        'n_corpus_motifs': len(gcn.corpus_motifs),

        'overall_novelty': {
            'mean': float(np.mean(results['overall'])),
            'std': float(np.std(results['overall'])),
            'median': float(np.median(results['overall'])),
            'min': float(np.min(results['overall'])),
            'max': float(np.max(results['overall'])),
            'q25': float(np.percentile(results['overall'], 25)),
            'q75': float(np.percentile(results['overall'], 75))
        },

        'structural_novelty': {
            'mean': float(np.mean(results['structural'])),
            'std': float(np.std(results['structural']))
        },

        'edge_type_novelty': {
            'mean': float(np.mean(results['edge_type'])),
            'std': float(np.std(results['edge_type']))
        },

        'bridging_novelty': {
            'mean': float(np.mean(results['bridging'])),
            'std': float(np.std(results['bridging']))
        },

        'per_graph_results': results['details']
    }

    print(f"\nOverall Novelty:")
    print(f"  Mean:   {stats['overall_novelty']['mean']:.4f} ± {stats['overall_novelty']['std']:.4f}")
    print(f"  Median: {stats['overall_novelty']['median']:.4f}")
    print(f"  Range:  [{stats['overall_novelty']['min']:.4f}, {stats['overall_novelty']['max']:.4f}]")
    print(f"  IQR:    [{stats['overall_novelty']['q25']:.4f}, {stats['overall_novelty']['q75']:.4f}]")

    print(f"\nComponent Novelty:")
    print(f"  Structural: {stats['structural_novelty']['mean']:.4f} ± {stats['structural_novelty']['std']:.4f}")
    print(f"  Edge-type:  {stats['edge_type_novelty']['mean']:.4f} ± {stats['edge_type_novelty']['std']:.4f}")
    print(f"  Bridging:   {stats['bridging_novelty']['mean']:.4f} ± {stats['bridging_novelty']['std']:.4f}")

    return stats


def main(args):
    # Evaluate the generation model
    stats = evaluate_generation_model(
        model_name=args.model,
        dataset_name=args.dataset,
        generated_graphs_path=args.input,
        corpus_size=args.corpus_size,
        k=args.k
    )

    # Save results
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w') as f:
        json.dump(stats, f, indent=2)

    print(f"\n{'='*80}")
    print(f"✓ Results saved to {output_path}")
    print(f"{'='*80}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Evaluate novelty on generated graphs')
    parser.add_argument('--model', type=str, required=True,
                        help='Model name (e.g., GraphRNN, GraphVAE, DiGress)')
    parser.add_argument('--dataset', type=str, required=True,
                        help='Dataset name (e.g., qm9, zinc)')
    parser.add_argument('--input', type=str, required=True,
                        help='Path to pickle file with generated graphs')
    parser.add_argument('--output', type=str, required=True,
                        help='Output JSON file for results')
    parser.add_argument('--corpus_size', type=int, default=900,
                        help='Size of training corpus')
    parser.add_argument('--k', type=int, default=3,
                        help='Motif size')

    args = parser.parse_args()
    main(args)
